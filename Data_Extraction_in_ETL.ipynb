{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : Describe different types of data sources used in ETL with suitable examples.\n",
        "\n",
        "- Answer 1\n",
        "\n",
        "Data sources in ETL (Extract, Transform, Load) are generally categorized by their structure:\n",
        "\n",
        "Structured Sources: Highly organized data in fixed formats.\n",
        "\n",
        "Example: Relational databases like MySQL, PostgreSQL, or Oracle.\n",
        "\n",
        "Semi-Structured Sources: Data that doesn't reside in a rational database but has some organizational properties like tags.\n",
        "\n",
        "Example: JSON files, XML files, or NoSQL databases like MongoDB.\n",
        "\n",
        "Unstructured Sources: Data with no predefined format or organization.\n",
        "\n",
        "Example: PDFs, images, emails, or social media posts.\n",
        "\n",
        "Flat Files: Simple text files containing data.\n",
        "\n",
        "Example: CSV or TSV files.\n",
        "\n",
        "Question 2 : What is data extraction? Explain its role in the ETL pipeline.\n",
        "\n",
        "- Answer 2\n",
        "\n",
        "Data extraction is the process of retrieving data from various (often disparate) sources for further processing.\n",
        "\n",
        "Role in ETL: It is the first and most critical stage. Its primary role is to act as a \"bridge\" between the source systems and the data warehouse. It ensures that the required data is gathered correctly without impacting the performance of the source systems, preparing it for the \"Transformation\" phase.\n",
        "\n",
        "Question 3 : Explain the difference between CSV and Excel in terms of extraction and ETL usage.\n",
        "\n",
        "- Answer 3\n",
        "\n",
        "\n",
        "Structure, Plain text; only data is stored.,\"Binary/XML; stores data, formulas, and formatting.\"\n",
        "\n",
        "Extraction, Very fast and consumes less memory.,Slower; requires specialized libraries to parse metadata.\n",
        "\n",
        "ETL Usage, Preferred for large datasets due to simplicity and speed.,\"Used for smaller, human-maintained reports or financial data.\"\n",
        "\n",
        "Question 4 : Explain the steps involved in extracting data from a relational database.\n",
        "\n",
        "- Answer 4\n",
        "\n",
        "Connection: Establishing a secure link via drivers like JDBC/ODBC.\n",
        "\n",
        "Identification: Determining which tables, views, or columns are needed.\n",
        "\n",
        "Querying: Executing SQL commands to pull data.\n",
        "\n",
        "Incremental/Full Load: Deciding whether to pull all data or only new records since the last extraction.\n",
        "\n",
        "Staging: Moving the pulled data into a temporary storage area for cleaning.\n",
        "\n",
        "\n",
        "Question 5 : Explain three common challenges faced during data extraction.\n",
        "\n",
        "- Answer 5\n",
        "\n",
        "Data Quality Issues: Dealing with missing values, inconsistent formats, or duplicate records at the source.\n",
        "\n",
        "Performance Impact: Extracting large volumes of data can slow down the production database for active users.\n",
        "\n",
        "Security/Access: Navigating firewall restrictions, encryption, and obtaining the correct permissions to sensitive data.\n",
        "\n",
        "\n",
        "Question 6 : What are APIs? Explain how APIs help in real-time data extraction.\n",
        "\n",
        "- Answer 6\n",
        "\n",
        "An API (Application Programming Interface) is a set of rules that allows two applications to talk to each other.\n",
        "\n",
        "Real-time role: APIs allow ETL tools to request data the moment it is updated (via Webhooks or REST calls). This enables \"streaming\" extraction, where data flows into the pipeline instantly rather than waiting for a scheduled batch at the end of the day.\n",
        "\n",
        "\n",
        "Question 7 : Why are databases preferred for enterprise-level data extraction?\n",
        "\n",
        "- Answer 7\n",
        "\n",
        "Scalability: Databases can handle millions of records efficiently.\n",
        "\n",
        "Data Integrity: They enforce relationships and constraints, ensuring \"cleaner\" source data.\n",
        "\n",
        "Efficiency: Features like Indexing and Query Optimization make retrieving specific subsets of data much faster than scanning flat files.\n",
        "\n",
        "Concurrency: Multiple ETL processes can often read from a database simultaneously without corrupting the file.\n",
        "\n",
        "Question 8 : What steps should an ETL developer take when extracting data from large CSV files (1GB+)?\n",
        "\n",
        "- Answer 8\n",
        "\n",
        "Chunking: Read the file in smaller \"chunks\" to prevent system memory crashes.\n",
        "\n",
        "Parallel Processing: Use multi-threading to process different sections of the file at the same time.\n",
        "\n",
        "Schema Inference: Explicitly define data types instead of letting the tool \"guess\" them, which saves processing power.\n",
        "\n",
        "Compression: Use compressed formats (like Gzip) to reduce I/O overhead during the transfer."
      ],
      "metadata": {
        "id": "WKEzKkkANKTK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRsujfKTNIRN"
      },
      "outputs": [],
      "source": []
    }
  ]
}